# üß† GLPI ‚Üî Qwen Bot (Ollama)

[![CI](https://github.com/<OWNER>/<REPO>/actions/workflows/ci.yml/badge.svg)](https://github.com/<OWNER>/<REPO>/actions/workflows/ci.yml)
[![E2E (live)](https://github.com/<OWNER>/<REPO>/actions/workflows/e2e-live.yml/badge.svg)](https://github.com/<OWNER>/<REPO>/actions/workflows/e2e-live.yml)
[![Publish (GHCR)](https://github.com/<OWNER>/<REPO>/actions/workflows/publish.yml/badge.svg)](https://github.com/<OWNER>/<REPO>/actions/workflows/publish.yml)

Assistant qui **lit** les tickets GLPI et **propose des r√©ponses** concr√®tes : diagnostic pas‚Äë√†‚Äëpas, questions cibl√©es, proc√©dures et conseils.
Le bot **s‚Äôarr√™te automatiquement pour CE ticket** si un **technicien** r√©pond ou si l‚Äôutilisateur **demande un humain**, et peut **reprendre** via `#resume-bot`.

- **Mod√®le** : Qwen 2.5 (via **Ollama**)
- **RAG l√©ger** : recherche de **cas similaires** (tickets r√©solus/clos) + **extraction automatique de mots‚Äëcl√©s** (alias: `wifi`, `vpn`, `ecran_noir`, ‚Ä¶)
- **S√©curit√©** : r√©ponses non destructives, respect de la confidentialit√© (RGPD), garde‚Äëfous

---

## ‚ú® Ce que √ßa apporte
- **Gain de temps** : incidents fr√©quents (eduroam/Wi‚ÄëFi, VPN, Outlook, imprimantes, ENT/Moodle‚Ä¶)
- **Qualit√© constante** : FR (ou EN si ticket en anglais), 4‚Äì6 √©tapes max, ton clair et courtois
- **Self‚Äëservice** : si l‚Äôutilisateur peut r√©soudre seul ‚Üí **r√©ponse publique** ; sinon ‚Üí **brouillon priv√©** au technicien
- **Z√©ro spam** : anti‚Äëdoublon (n‚Äôenvoie pas 2√ó la m√™me r√©ponse), **opt‚Äëout par ticket**

---

## üèóÔ∏è Conception
- **GLPI API**
  - Liste des tickets actifs (statuts 1/2/3, param√©trable), lecture des suivis, cr√©ation de suivis **public/priv√©**
  - D√©tection : ¬´¬†je veux parler √† un humain¬†¬ª ‚Üí opt‚Äëout du ticket ; r√©ponse **technicien** ‚Üí opt‚Äëout du ticket ; suivi priv√© `#resume-bot` ‚Üí **reprise**
- **RAG / Similarit√©**
  - Index des tickets **r√©solus/clos** (pagination GLPI, cache disque)
  - Score pond√©r√© : **mots‚Äëcl√©s du titre** (bigrams + alias), contenu, + **keywords extraits**
  - Le mod√®le voit des **r√©sum√©s** de r√©solutions proches dans son prompt
- **G√©n√©ration (Ollama)**
  - `/api/chat` (JSON schema) ‚Üí fallback `/api/generate`
  - Sortie JSON stricte :
    ```json
    { "reply":"...", "confidence":0, "tags":["..."], "close_candidate":false, "audience":"user|technician", "public_reply":true|false }
    ```
- **Ops / perfs**
  - Polling adaptatif, limitation du nombre de tickets par cycle, cache d‚Äôindex similaire
  - Docker Compose + image GHCR via GitHub Actions

---

## üöÄ D√©marrage rapide

### Option A ‚Äî Docker Compose (recommand√©)
1. Copier l‚Äôexemple d‚Äôenvironnement :
   ```bash
   cp .env.example .env
   # √âditer .env et renseigner GLPI_URL / GLPI_APP_TOKEN / GLPI_USER_TOKEN
   ```
2. Lancer :
   ```bash
   docker compose up -d --build
   ```
3. Ex√©cuter une passe unique :
   ```bash
   docker compose run --rm bot python app.py --once
   ```
> Le service `ollama` √©coute sur `11434`. Le bot persiste `state.json` et `similar_index.json` dans le volume `bot_data` (`/data`).

### Option B ‚Äî Image publi√©e (GHCR)
Apr√®s passage du workflow **Publish**, tirer l‚Äôimage :
```bash
docker pull ghcr.io/<OWNER>/<REPO>:latest
```
Exemple `docker-compose.yml` (image GHCR) :
```yaml
services:
  ollama:
    image: ollama/ollama:latest
    ports: [ "11434:11434" ]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/tags"]
      interval: 5s
      timeout: 3s
      retries: 20
    volumes:
      - ollama_data:/root/.ollama

  bot:
    image: ghcr.io/<OWNER>/<REPO>:latest
    depends_on:
      ollama:
        condition: service_healthy
    env_file:
      - .env
    environment:
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL-http://ollama:11434}
      MODEL_NAME: ${MODEL_NAME-qwen2.5:1.5b-instruct}
      STATE_FILE: /data/state.json
      SIMILAR_INDEX_CACHE: /data/similar_index.json
    volumes:
      - bot_data:/data
    restart: unless-stopped

volumes:
  ollama_data: {}
  bot_data: {}
```

---

## üîß Configuration (.env)
Variables principales (voir `.env.example`) :
```
GLPI_URL=https://votre.glpi.tld/apirest.php
GLPI_APP_TOKEN=...
GLPI_USER_TOKEN=...

# Ollama
OLLAMA_BASE_URL=http://ollama:11434
MODEL_NAME=qwen2.5:1.5b-instruct

# Perf
POLL_SECONDS=20
ADAPTIVE_POLL=1
MAX_HISTORY=8
MAX_TICKETS_PER_CYCLE=10
OLLAMA_KEEP_ALIVE=5m

# Similarit√© (historique + poids titre)
SIMILAR_LOOKBACK_LIMIT=5000
SIMILAR_PAGE_SIZE=200
SIMILAR_MAX_PAGES=25
SIMILAR_FETCH_ORDER=desc
TITLE_KEYWORD_WEIGHT=0.65
CONTENT_WEIGHT=0.35
MIN_TITLE_OVERLAP=1
SIMILAR_INDEX_CACHE=/data/similar_index.json
SIMILAR_CACHE_TTL_MIN=60

# Keywords
KEYWORDS_TOP_K=8
KEYWORD_SIM_WEIGHT=0.25

# State (persist√©s)
STATE_FILE=/data/state.json
```

---

## ‚úÖ Tests
- **Unitaires (sans GLPI/Ollama)** :
  ```bash
  pip install -r requirements.txt
  pytest -q
  ```
- **E2E (live)** : onglet **Actions** ‚Üí workflow **E2E (live)** ‚Üí *Run workflow*  
  *(secrets requis : `GLPI_URL`, `GLPI_APP_TOKEN`, `GLPI_USER_TOKEN` et √©ventuellement `GLPI_USER_TOKEN_TECH` / `GLPI_USER_TOKEN_USER`)*

---

## üõ†Ô∏è Workflows GitHub Actions
- **CI** : `.github/workflows/ci.yml` ‚Äî tests unitaires mock  
- **E2E (live)** : `.github/workflows/e2e-live.yml` ‚Äî passe unique avec service Ollama dans le runner  
- **Publish (GHCR)** : `.github/workflows/publish.yml` ‚Äî build multi‚Äëarch & push sur `ghcr.io/<OWNER>/<REPO>`

---

## üÜò D√©pannage
- Le bot ne r√©pond pas ‚Üí v√©rifier `.env` (URL/API GLPI, tokens), l‚Äôacc√®s r√©seau depuis le conteneur, logs `bot`
- Ollama indisponible ‚Üí `docker logs ollama` ; s‚Äôil manque un mod√®le, il sera **pull** au d√©marrage
- GHCR priv√© ‚Üí login : `echo $PAT | docker login ghcr.io -u <USER> --password-stdin` (PAT `read:packages`)

---

